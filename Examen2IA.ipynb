{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Segundo examen Práctico de IA (Semestre 2025-1)\n",
        "\n",
        "**Tema:** Redes Neuronales Recurrentes (RNNs) y Modelado Generativo (Autoencoders y GANs)\n",
        "\n",
        "**Instrucciones:**\n",
        "- Resuelve los siguientes dos ejercicios\n",
        "- Puedes utilizar Python y las librerías estándar de aprendizaje automático como TensorFlow.\n",
        "- Asegúrate de comentar tu código donde sea necesario para explicar tus decisiones.\n",
        "\n",
        "---\n",
        "\n",
        "## Datos Proporcionados:\n",
        "\n",
        "Para completar este examen, se te proporcionan los siguientes datos:\n",
        "\n",
        "- **Ejercicio 1:** Un archivo de texto `cuentos.txt` que contiene una colección de cuentos cortos escritos especialmente para este examen.\n",
        "- **Ejercicio 2:** Un conjunto de imágenes `simbolos.npy`, que contiene 1000 imágenes de 28x28 píxeles en escala de grises de símbolos abstractos generados para este examen.\n",
        "\n",
        "Los datos están disponibles en el mismo directorio que este examen.\n",
        "\n",
        "**Nota:** A continuación, se incluyen instrucciones para generar estos datos en caso de que no los tengas disponibles.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0hs1OdXvCD9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de `cuentos.txt` y visualización de algunos cuentos:"
      ],
      "metadata": {
        "id": "EG3FxpU-CImt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWo8eVVnCAp8",
        "outputId": "07e6cb64-e1c7-4e6c-d742-6bb0f1da6978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Muestra de cuentos generados ---\n",
            "\n",
            "\n",
            "    Cuento 1:\n",
            "    Había una vez Hansel que vivía en una isla desierta. Un día, encontró una varita mágica que cambió su vida para siempre. A través de aventuras y desafíos, Hansel aprendió que el valor y la amistad son las fuerzas más poderosas.\n",
            "    \n",
            "\n",
            "    Cuento 2:\n",
            "    Había una vez Hansel que vivía en un bosque encantado. Un día, encontró un mapa del tesoro que cambió su vida para siempre. A través de aventuras y desafíos, Hansel aprendió que la verdadera riqueza está en el corazón.\n",
            "    \n",
            "\n",
            "    Cuento 3:\n",
            "    Había una vez Hansel que vivía en una montaña misteriosa. Un día, encontró una varita mágica que cambió su vida para siempre. A través de aventuras y desafíos, Hansel aprendió que la verdadera riqueza está en el corazón.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Generar 100 cuentos variados y guardarlos en 'cuentos.txt'\n",
        "import random\n",
        "\n",
        "# Lista de personajes, lugares, objetos y moralejas para crear variaciones\n",
        "personajes = ['Alicia', 'Pedro', 'María', 'Juan', 'Lucía', 'Carlos', 'El Gato con Botas', 'Caperucita Roja', 'Hansel', 'Gretel']\n",
        "lugares = ['un bosque encantado', 'una ciudad futurista', 'un reino submarino', 'una montaña misteriosa', 'una isla desierta']\n",
        "objetos = ['una varita mágica', 'un mapa del tesoro', 'una espada legendaria', 'un libro de hechizos', 'una lámpara mágica']\n",
        "moralejas = [\n",
        "    'la verdadera riqueza está en el corazón',\n",
        "    'el valor y la amistad son las fuerzas más poderosas',\n",
        "    'la honestidad es la mejor política',\n",
        "    'nunca juzgues un libro por su portada',\n",
        "    'la perseverancia conduce al éxito'\n",
        "]\n",
        "\n",
        "cuentos_list = []\n",
        "\n",
        "for i in range(100):  # Generar 100 cuentos\n",
        "    personaje = random.choice(personajes)\n",
        "    lugar = random.choice(lugares)\n",
        "    objeto = random.choice(objetos)\n",
        "    moraleja = random.choice(moralejas)\n",
        "\n",
        "    cuento = f\"\"\"\n",
        "    Cuento {i+1}:\n",
        "    Había una vez {personaje} que vivía en {lugar}. Un día, encontró {objeto} que cambió su vida para siempre. A través de aventuras y desafíos, {personaje} aprendió que {moraleja}.\n",
        "    \"\"\"\n",
        "    cuentos_list.append(cuento)\n",
        "\n",
        "# Unir todos los cuentos en un solo texto\n",
        "cuentos = '\\n'.join(cuentos_list)\n",
        "\n",
        "# Guardar el texto en 'cuentos.txt'\n",
        "with open('cuentos.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(cuentos)\n",
        "\n",
        "# Mostrar los primeros 3 cuentos\n",
        "print('--- Muestra de cuentos generados ---\\n')\n",
        "for cuento in cuentos_list[:3]:\n",
        "    print(cuento)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de `simbolos.npy` y visualización de algunas imágenes:"
      ],
      "metadata": {
        "id": "wrjFEmgPCRZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar 1000 imágenes de símbolos abstractos y guardarlas en 'simbolos.npy'\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.draw import random_shapes\n",
        "\n",
        "simbolos = []\n",
        "\n",
        "for _ in range(2000):\n",
        "    img, _ = random_shapes(\n",
        "        (28, 28),\n",
        "        max_shapes=5,\n",
        "        min_shapes=1,\n",
        "        max_size=20,\n",
        "        num_channels=1,  # Cambiado 'multichannel' a 'num_channels'\n",
        "        intensity_range=((0, 255)),\n",
        "        allow_overlap=True\n",
        "    )\n",
        "    img = img.astype('float32').squeeze() / 255.0  # Asegurar que la imagen es 2D\n",
        "    simbolos.append(img)\n",
        "\n",
        "simbolos = np.array(simbolos)\n",
        "\n",
        "# Guardar las imágenes en 'simbolos.npy'\n",
        "np.save('simbolos.npy', simbolos)\n",
        "\n",
        "# Visualizar los primeros 5 símbolos\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    axes[i].imshow(simbolos[i], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "5J-6kp2WCSaY",
        "outputId": "4a4cfbac-6972-4cb9-ab82-73f18fd4afab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACZCAYAAABHTieHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFFUlEQVR4nO3dMU4jWRRA0faIxCtiHwSQmD2QeQ1k7MEkOGAfXpETJE/cbmumGtd12dQ5WSMof4TV4urx9BeHw+HwCwAAYGT/TH0AAADgZxIbAABAQmwAAAAJsQEAACTEBgAAkBAbAABAQmwAAAAJsQEAACTupj7AlF5fX7/1dev1euSTAADAz2OyAQAAJMQGAACQEBsAAEBCbAAAAAmxAQAAJMQGAACQEBsAAEBCbAAAAAmxAQAAJGZ9gzhA7f39PX3+arVKnw8A5zDZAAAAEmIDAABIiA0AACAx652N9Xo99REY6P7+fuoj3Lzdbjf1EQCAmTHZAAAAEmIDAABIiA0AACAhNgAAgITYAAAAEmIDAABIiA0AACAhNgAAgITYAAAAErO+QRyAy1ksFunzD4dD+nzm6/i9670Gw5lsAAAACbEBAAAkxAYAAJAQGwAAQMKCOACJeiF8yOtZ5AWYlskGAACQEBsAAEBCbAAAAAk7GwCh1Wo19REu4tL7GUPZ4+BvDXkve1/BcCYbAABAQmwAAAAJsQEAACTEBgAAkLAgDsDNWS6Xgz5vv9/HJwHgv5hsAAAACbEBAAAkxAYAAJAQGwAAQMKCOAAwS0NuCz/nWW4VB5MNAAAgIjYAAICE2AAAABJiAwAASFgQh5mwvAgAXJrJBgAAkBAbAABAQmwAAAAJsQEAACQsiMOMWRoH5mLM28LPeU3/xzI3JhsAAEBCbAAAAAmxAQAAJPKdjefn5/olRrPZbKY+Akxuu92mz398fEyfzzRO/R36FH8jf8zfx8/XNbz/TrHHcR2GvD/8XMZhsgEAACTEBgAAkBAbAABAQmwAAAAJl/oBv3l6evrjYx8fHxOchNp+v//t38vlctTnHy9Xjrmwe3z2U6/HvHk/wHUw2QAAABJiAwAASIgNAAAgITYAAICEBXEALsLCLsD8mGwAAAAJsQEAACTEBgAAkBAbAABAwoI48L+ObxV3o/jtOXXj9pDPGftWcQDmxWQDAABIiA0AACAhNgAAgITYAAAAEhbEgb92vDD+65elceC6PTw8/PGxz8/PCU4C82KyAQAAJMQGAACQEBsAAEDCzgbMhJ2K+Rhygd85z3LRHz+FPQ7omWwAAAAJsQEAACTEBgAAkBAbAABAwoI4APDjWfyGaZhsAAAACbEBAAAkxAYAAJAQGwAAQMKC+MTu7sb7EXx9fY32LOA2jHlb+Dmv6VZxAE4x2QAAABJiAwAASIgNAAAgITYAAIBEviC+2Wzql7gZYy6Dn/N8i+Rwm6ZYBh/K0jgAp5hsAAAACbEBAAAkxAYAAJBwqR83YbfbTX2E3Ha7nfoIXDk7EADcGpMNAAAgITYAAICE2AAAABJiAwAASCwOh8Nh6kOcY7FYfPtrL/2t15f6DeVSPwBgzob8/njjvyJfDZMNAAAgITYAAICE2AAAABJiAwAASFzHxvJMDF3MHnOR3DI4AABTMdkAAAASYgMAAEiIDQAAICE2AACAhBvE+dFeXl6+/bVvb2+jnQMAYI5MNgAAgITYAAAAEmIDAABIiA0AACBx8zeIW/IGAIDrZLIBAAAkxAYAAJAQGwAAQEJsAAAACbEBAAAkxAYAAJAQGwAAQEJsAAAAicXBrXgAAEDAZAMAAEiIDQAAICE2AACAhNgAAAASYgMAAEiIDQAAICE2AACAhNgAAAASYgMAAEiIDQAAICE2AACAhNgAAAASYgMAAEiIDQAAIPEvygOtoiNixj4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Ejercicio 1: Generación de Texto con Redes Neuronales Recurrentes\n",
        "\n",
        "Utilizando el archivo `cuentos.txt`, tu tarea es construir un modelo de red neuronal recurrente que pueda generar texto en el estilo de los cuentos proporcionados.\n",
        "\n",
        "### Tareas:\n",
        "\n",
        "1. **Preprocesamiento de Datos:**\n",
        "\n",
        "   - Carga el texto desde `cuentos.txt` y realiza el preprocesamiento necesario.\n",
        "   - Continúa con las instrucciones especificadas en el enunciado."
      ],
      "metadata": {
        "id": "bniA8sJdCb-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Cargar el texto desde 'cuentos.txt'\n",
        "with open('cuentos.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Preprocesar el texto: convertirlo a minúsculas\n",
        "text = text.lower()\n",
        "\n",
        "# Crear un conjunto de palabras únicas en el texto\n",
        "words = text.split()\n",
        "vocab = sorted(set(words))\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "# Convertir el texto en una secuencia de enteros basados en las palabras\n",
        "text_as_int = np.array([word2idx[word] for word in words])\n",
        "\n",
        "# Definir la longitud de las secuencias\n",
        "seq_length = 5  # Longitud de las secuencias de palabras\n",
        "examples_per_epoch = len(words) // (seq_length + 1)\n",
        "\n",
        "# Crear ejemplos y etiquetas para el entrenamiento\n",
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "KznxW53QCdVT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño del buffer y batch\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Definir el tamaño del vocabulario y las dimensiones de la capa de embedding\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "# Crear el modelo de SimpleRNN\n",
        "def build_model(vocab_size, embedding_dim, rnn_units):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_shape=(None,)),\n",
        "        tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, stateful=False),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
      ],
      "metadata": {
        "id": "mf6ZkMBpJMi_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir número de épocas\n",
        "EPOCHS = 10\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7c7NtzpJQgo",
        "outputId": "e79cf76c-e28b-4668-e70b-c14880c1ebe2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.7029\n",
            "Epoch 2/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5129\n",
            "Epoch 3/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6362 \n",
            "Epoch 4/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2351 \n",
            "Epoch 5/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9599 \n",
            "Epoch 6/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8334 \n",
            "Epoch 7/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6945 \n",
            "Epoch 8/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6289 \n",
            "Epoch 9/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5743 \n",
            "Epoch 10/10\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5345 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar texto basado en palabras\n",
        "def generate_text(model, start_string):\n",
        "    num_generate = 50  # Número de palabras a generar\n",
        "    input_eval = [word2idx[word] for word in start_string.split()]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    # Restablecer los estados de la capa SimpleRNN\n",
        "    model.layers[1].reset_states()  # Suponiendo que la capa SimpleRNN es la segunda en el modelo\n",
        "\n",
        "    # Generar texto palabra por palabra\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "    return start_string + ' ' + ' '.join(text_generated)\n",
        "\n",
        "# Ejemplo de generación de texto\n",
        "print(generate_text(model, start_string=\"había una vez\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biKXYIW9JTn4",
        "outputId": "894add9d-5207-4a74-f633-a6b85f08daff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "había una vez gretel mapa el corazón. espada que vivía en un día, encontró una vez 100: 16: 56: y desafíos, 41: al desafíos, encontró un día, encontró un día, encontró una vez pedro 63: 85: 17: 99: 46: juzgues un libro su vida para siempre. a través de aventuras varita y desafíos,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Ejercicio 2: Reconstrucción y Generación de Imágenes con Autoencoders y GANs\n",
        "\n",
        "Utilizando el conjunto de imágenes `simbolos.npy`, que contiene 1000 imágenes de símbolos abstractos, explora el uso de Autoencoders y Generative Adversarial Networks (GANs) para la reconstrucción y generación de imágenes.\n",
        "\n",
        "### Tareas:\n",
        "\n",
        "1. **Autoencoder:**\n",
        "\n",
        "   - Carga las imágenes desde `simbolos.npy` y normalízalas al rango [0, 1].\n",
        "   - Continúa con las instrucciones especificadas en el enunciado."
      ],
      "metadata": {
        "id": "daMGnHaYCgMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Cargar las imágenes desde 'simbolos.npy'\n",
        "simbolos = np.load('simbolos.npy')\n"
      ],
      "metadata": {
        "id": "D-FD_MikCgiJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Modelo de Autoencoder basado en convoluciones\n",
        "input_img = tf.keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "# Codificador\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Decodificador\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "\n",
        "# Compilar el modelo\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Entrenar el Autoencoder\n",
        "autoencoder.fit(simbolos, simbolos, epochs=100, batch_size=64, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRtxTT6CEMuY",
        "outputId": "f7d94cc5-f6d6-4e89-fa42-5b4f8b603a4c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.4826 - val_loss: 0.2507\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1966 - val_loss: 0.1427\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1269 - val_loss: 0.1213\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1124 - val_loss: 0.1128\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1067 - val_loss: 0.1080\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1031 - val_loss: 0.1053\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0992 - val_loss: 0.1002\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0970 - val_loss: 0.0972\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0914 - val_loss: 0.0951\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0918 - val_loss: 0.0927\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0887 - val_loss: 0.0905\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0859 - val_loss: 0.0895\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0884 - val_loss: 0.0897\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0867 - val_loss: 0.0900\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0896 - val_loss: 0.0875\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0845 - val_loss: 0.0868\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0828 - val_loss: 0.0863\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0837 - val_loss: 0.0863\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0821 - val_loss: 0.0856\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - val_loss: 0.0851\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0811 - val_loss: 0.0853\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - val_loss: 0.0851\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0868 - val_loss: 0.0840\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - val_loss: 0.0837\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0816 - val_loss: 0.0833\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0805 - val_loss: 0.0839\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0844 - val_loss: 0.0833\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0807 - val_loss: 0.0833\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0802 - val_loss: 0.0840\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0821 - val_loss: 0.0822\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0818 - val_loss: 0.0824\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0824 - val_loss: 0.0823\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0813 - val_loss: 0.0816\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0806 - val_loss: 0.0814\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0771 - val_loss: 0.0813\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0789 - val_loss: 0.0811\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0773 - val_loss: 0.0811\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0763 - val_loss: 0.0805\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0765 - val_loss: 0.0805\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0783 - val_loss: 0.0805\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0809 - val_loss: 0.0804\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0801 - val_loss: 0.0802\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0773 - val_loss: 0.0799\n",
            "Epoch 44/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0758 - val_loss: 0.0797\n",
            "Epoch 45/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0762 - val_loss: 0.0794\n",
            "Epoch 46/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0761 - val_loss: 0.0794\n",
            "Epoch 47/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0789 - val_loss: 0.0812\n",
            "Epoch 48/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0794 - val_loss: 0.0793\n",
            "Epoch 49/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0778 - val_loss: 0.0794\n",
            "Epoch 50/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0765 - val_loss: 0.0794\n",
            "Epoch 51/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0777 - val_loss: 0.0787\n",
            "Epoch 52/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0733 - val_loss: 0.0792\n",
            "Epoch 53/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0765 - val_loss: 0.0787\n",
            "Epoch 54/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0767 - val_loss: 0.0784\n",
            "Epoch 55/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0761 - val_loss: 0.0780\n",
            "Epoch 56/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0747 - val_loss: 0.0795\n",
            "Epoch 57/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0767 - val_loss: 0.0782\n",
            "Epoch 58/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0790 - val_loss: 0.0779\n",
            "Epoch 59/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0750 - val_loss: 0.0777\n",
            "Epoch 60/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0767 - val_loss: 0.0783\n",
            "Epoch 61/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0770 - val_loss: 0.0781\n",
            "Epoch 62/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0727 - val_loss: 0.0775\n",
            "Epoch 63/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0726 - val_loss: 0.0772\n",
            "Epoch 64/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0745 - val_loss: 0.0774\n",
            "Epoch 65/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0746 - val_loss: 0.0773\n",
            "Epoch 66/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0746 - val_loss: 0.0770\n",
            "Epoch 67/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0728 - val_loss: 0.0769\n",
            "Epoch 68/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0759 - val_loss: 0.0815\n",
            "Epoch 69/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0739 - val_loss: 0.0776\n",
            "Epoch 70/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0737 - val_loss: 0.0770\n",
            "Epoch 71/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0760 - val_loss: 0.0770\n",
            "Epoch 72/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0758 - val_loss: 0.0768\n",
            "Epoch 73/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0758 - val_loss: 0.0764\n",
            "Epoch 74/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0753 - val_loss: 0.0764\n",
            "Epoch 75/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0727 - val_loss: 0.0766\n",
            "Epoch 76/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0736 - val_loss: 0.0764\n",
            "Epoch 77/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0752 - val_loss: 0.0762\n",
            "Epoch 78/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0738 - val_loss: 0.0764\n",
            "Epoch 79/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0748 - val_loss: 0.0769\n",
            "Epoch 80/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0728 - val_loss: 0.0761\n",
            "Epoch 81/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0764 - val_loss: 0.0759\n",
            "Epoch 82/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0742 - val_loss: 0.0758\n",
            "Epoch 83/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0734 - val_loss: 0.0759\n",
            "Epoch 84/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0755 - val_loss: 0.0756\n",
            "Epoch 85/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0723 - val_loss: 0.0759\n",
            "Epoch 86/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0741 - val_loss: 0.0799\n",
            "Epoch 87/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0799 - val_loss: 0.0774\n",
            "Epoch 88/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0736 - val_loss: 0.0763\n",
            "Epoch 89/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0756 - val_loss: 0.0758\n",
            "Epoch 90/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0743 - val_loss: 0.0755\n",
            "Epoch 91/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0736 - val_loss: 0.0755\n",
            "Epoch 92/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0718 - val_loss: 0.0761\n",
            "Epoch 93/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0736 - val_loss: 0.0761\n",
            "Epoch 94/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0754 - val_loss: 0.0753\n",
            "Epoch 95/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0735 - val_loss: 0.0752\n",
            "Epoch 96/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0713 - val_loss: 0.0751\n",
            "Epoch 97/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0725 - val_loss: 0.0755\n",
            "Epoch 98/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0747 - val_loss: 0.0752\n",
            "Epoch 99/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0741 - val_loss: 0.0755\n",
            "Epoch 100/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0715 - val_loss: 0.0754\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79465c3839a0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generar imágenes reconstruidas\n",
        "reconstructed_imgs = autoencoder.predict(simbolos)\n",
        "\n",
        "# Mostrar algunas imágenes originales y sus reconstrucciones\n",
        "n = 5\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n):\n",
        "    # Mostrar imagen original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(simbolos[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Mostrar imagen reconstruida\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed_imgs[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "SAUREyF5ETMb",
        "outputId": "82e8fe52-cb32-45e5-9720-d390549d0d7b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}